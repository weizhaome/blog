title: 机器翻译实习总结
author: weizhao
date: 2019-07-05 14:53:13
tags:
---
# 机器翻译（中英）实习总结

2015年5月-7月参与机器翻译项目，接下来将对这两个月的收获进行总结。
## Transform模型的理解
> 谷歌提出的transformer模型，使用self-attention层和全连接层实现了很好的机器翻译BLEU结果，没有采用传统的RNN/LSTM/GRU结构，解决了长依赖和训练并行度问题，降低了计算的复杂度。关于论文的详细讲解可以参考：https://www.jianshu.com/p/ef41302edeef等的讲解。
- **Attention机制**  
** 简介:**
在传统的encoder-decoder模型中，对于句子对<X,Y>,，我们输入给定句子X=<x1,x2...xm> 经过encoder进行编码后转化为中间语义表示为C=F(x<sub>1</sub>,x<sub>2</sub>...x<sub>m</sub>)，解码器decoder的任务是根据句子的中间语义表示C和之前已经生成的历史信息y<sub>1</sub>,y<sub>2</sub>...y<sub>i-1</sub>)来生成i时刻要生成的单词yi=G(C,y<sub>1</sub>,y<sub>2</sub>...y<sub>i-1</sub>)。   
这样的框架因为对于预测每一个yi对应的语义编码C都是一样的，无法完全表示整个序列的信息，而且先输入的内容携带的信息会被后面的信息覆盖掉。attention机制将用来解决该问题，在预测时不仅关注全局语义编码向量C，而且增加‘注意力范围’，来表示接下来输出词时候要重点关注输入序列的哪些部分。  
![attention](/images/nmt/Attention.png)  
此时生成目标句子单词的过程就成了下面的形式:  
y<sub>1</sub>=f1(C<sub>1</sub>)  
y<sub>2</sub>=f1(C<sub>2</sub>,y<sub>1</sub>)  
y<sub>3</sub>=f1(C<sub>3</sub>,y<sub>1</sub>,y<sub>2</sub>)  
**计算过程：**1、计算encoder每个结点与decoderd当前结点的Score值：
\[s1,s2,s3,s4]=softmax(dot(\[x1,x2,x3,x4],yi-1))  
2、当前结点的语义编码为Ci=s1\*x1+s2\*x2+s3\*x3+s4*x4
- **self-attention机制**  
self-attention是attention机制的一种，attention是输入输出的权重，而self-attention是自己对自己的权重，计算公式如下图：  
![self-attention](/images/nmt/self-attention.png)  
Q(queries)，K (keys) and V(Values)， 其中 Key and values 一般对应同样的 vector， K=V 而Query vecotor  是对应目标句子的 word vector。
- **论文代码的理解**  
所查看的代码使用tensorflow框架，其各个函数的调用以及参数传递如下图：  
![transformer](/images/nmt/transformer-function.png)  
这里使用了TensorFlow中的高级API：Estimator、Experiment,具体讲解可以参考：https://www.jiqizhixin.com/articles/2017090901

![API](/images/nmt/API.png)  

##  shell命令总结 
语料库中包含两千多万条训练数据，经过了一系列的过滤得到一个干净整洁的训练预料。在过程中主要用shell对其进行处理，提高处理的速度和效率。接下来将对过程中使用到的shell命令进行总结。
- **awk** 
awk用来对文本进行较复杂格式处理,https://www.cnblogs.com/silence-hust/p/4534606.html 中对awk做了较为详细的汇总。
> 1、统计英文文本中每行的单词个数可以用命令：
awk -F ' ' 'BEGIN{print "count", "lineNum"}{print NF "\t" NR}' filename 其中，-F 表示分隔符，NF是awk的内置函数表示读取记录的域的个数，NR表示已经读取的记录数，文件的每行相当于记录，文件的每个字段，也就是每个单词相当于记录的域。其中BEGIN后面所跟的命令只会在遍历文件前执行一次，END命令也会在遍历完文件后执行一次。  
> 2、删除英文文本中单词数量少于5的行可以用命令：
awk -F ' ' '{if (NF < 5) {next} {print}}' filename 其中，if会用来做判断，如果满足条件将会调用{next}直接跳到下一行，如果不满足会执行{print}输出当前行。  
>3、awk '!a[$0]++' filename 将会去除文件中重复的行，'!'表示非，$0表示当前行，a[$0]以当前行为数据下标建立数组a,a[$0]++表示给数组a赋值，a[$0]+=1。详细解释可以参照：https://www.cnblogs.com/irockcode/p/7044646.html  
- **grep**  
grep更适合单纯的查找和匹配文本， https://blog.csdn.net/liupeifeng3514/article/details/79880878 中对grep命令做了详细的介绍。
> 1、统计出现文本中出现某种模式n词以上的行：grep -E '(pattern){n,}' filename 其中-E表示使用正则表达式，(pattern)是要找的某种模式，用正则表达式进行匹配，{n,}表示连续出现n次以上。  
> 2、grep -f file1 file2 过滤掉文件2中包含文件1的内容。 如果添加 -v 可以实现反向输出，即可以输出两个文件重复的行。
- **shell效率问题**  
因为数据量在千万级，所以一个好的处理策略要比处理工具更加重要，本次项目是中-英的机器翻译，所以在对数据筛选时要同时兼顾两个文件，即源端数据文本和目标端数据文本，在去重的过程中一开始的想法是将源段出现重复和行号找出后保存在文件中，之后删除目标段对应的行，但速度会很慢。  
后来利用paste 先将两个文件用TAB作为分隔符左右拼接起来：
paste -d\\t file1 file2 > tempfile  
之后利用awk去除重复行：  
awk -F '\\t' '!a[$1]++' tempfile > uniqfile 这里的$1表示第一列即源段文本。  
最后用cut将其分开。  
后来看到了https://blog.csdn.net/dubendi/article/details/79103170，关于如何提高shell效率的博客。
- **shell输出重定向**
![stdout](/images/nmt/stdout.png)
在此次项目中原始训练集在一个文件中，按照一行源段，一行目标端，一行空格来放。所以首要任务是将其分成两个文件，这里利用重定向写了一小段shell脚本：
```#!/usr/bin/perl
$n=0;
while(<>){
chomp();
if($n%3==0){print STDOUT "$_\n";}
if($n%3==1){print STDERR "$_\n";}
$n+=1;
}
```  
初始化变量n,用来表示读取的是源段还会目标端的行，按行读取文件，当是空行时用chomp()删除改行，如果不是判断n是否能被3整除，如果可以的话用标准输出将其输出，如果不是的话用标准错误输出将其输出，最后在命令行运行该脚本：$./vcut.pl sg-mt.txt 2> train.en 1> train.zh 实现将两中文本分开。